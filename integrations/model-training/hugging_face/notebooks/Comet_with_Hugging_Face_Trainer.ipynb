{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG0Hxf-nFKG2"
      },
      "source": [
        "<img src=\"https://cdn.comet.ml/img/notebook_logo.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF2cWLIyzgxe"
      },
      "source": [
        "[Hugging Face](https://huggingface.co/docs) is a community and data science platform that provides tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies. Primarily known for their `transformers` library, Hugging Face has helped democratized access to these models by providing a unified API to train and evaluate a number of popular models for NLP. \n",
        "\n",
        "Comet integrates with Hugging Face's `Trainer` object, allowing you to log your model parameters, metrics, and assets such as model checkpoints. Learn more about our integration [here](https://www.comet.com/docs/v2/integrations/ml-frameworks/huggingface/) \n",
        "\n",
        "Curious about how Comet can help you build better models, faster? Find out more about [Comet](https://www.comet.com/site/products/ml-experiment-tracking/?utm_campaign=transformers&utm_medium=colab) and our [other integrations](https://www.comet.ml/docs/v2/integrations/overview/)\n",
        "\n",
        "\n",
        "Get a preview for what's to come. Check out a completed experiment created from this notebook [here](https://www.comet.com/examples/comet-examples-transformers-trainer/3992ddee441f446bbb65c3cc4c8bd33b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUdcwLCWIz2n"
      },
      "source": [
        "# Install Comet and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nGDxYlCEKHMt"
      },
      "outputs": [],
      "source": [
        "# %pip install comet_ml torch datasets transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VaHie31I3r_"
      },
      "source": [
        "# Initialize Comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8twVwYTIKK_x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: Comet API key is valid\n"
          ]
        }
      ],
      "source": [
        "import comet_ml\n",
        "\n",
        "comet_ml.init(project_name=\"comet-examples-transfomers-trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-isTv4sTU1n"
      },
      "source": [
        "# Set Model Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2R9ckDVbTWaN"
      },
      "outputs": [],
      "source": [
        "PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\"\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJRfuqCXI948"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AD_Ejs6sKTRl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 4.31kB [00:00, 669kB/s]                    \n",
            "Downloading metadata: 2.17kB [00:00, 586kB/s]                    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /Users/toon.weyens/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 84.1M/84.1M [00:14<00:00, 5.77MB/s]\n",
            "                                                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset imdb downloaded and prepared to /Users/toon.weyens/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 241.38it/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDXLkGRnI_jn"
      },
      "source": [
        "# Setup Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DI24QdbPKoO9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 8.90kB/s]\n",
            "Downloading config.json: 100%|██████████| 483/483 [00:00<00:00, 127kB/s]\n",
            "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 725kB/s] \n",
            "Downloading tokenizer.json: 100%|██████████| 455k/455k [00:00<00:00, 908kB/s] \n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "flBnDDk4M8Yo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [00:11<00:00,  2.16ba/s]\n",
            "100%|██████████| 25/25 [00:11<00:00,  2.17ba/s]\n",
            "100%|██████████| 50/50 [00:24<00:00,  2.07ba/s]\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vhN2owT-tOcv"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMX-ysNYOCN4"
      },
      "source": [
        "# Create Sample Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBYSBPoZOEFq"
      },
      "source": [
        "For this guide, we are only going to sample 200 examples from our dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "urC6KnPbNET3"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=SEED).select(range(200))\n",
        "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=SEED).select(range(200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU4ggdjCJJT1"
      },
      "source": [
        "# Setup Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sythIXibNmFD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading pytorch_model.bin: 100%|██████████| 256M/256M [00:06<00:00, 41.5MB/s] \n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    PRE_TRAINED_MODEL_NAME, num_labels=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXUjopsZpo-D"
      },
      "source": [
        "# Setup Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gl6m6IJUporu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "def get_example(index):\n",
        "    return eval_dataset[index][\"text\"]\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    experiment = comet_ml.get_global_experiment()\n",
        "\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\"\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    if experiment:\n",
        "        epoch = int(experiment.curr_epoch) if experiment.curr_epoch is not None else 0\n",
        "        experiment.set_epoch(epoch)\n",
        "        experiment.log_confusion_matrix(\n",
        "            y_true=labels,\n",
        "            y_predicted=preds,\n",
        "            file_name=f\"confusion-matrix-epoch-{epoch}.json\",\n",
        "            labels=[\"negative\", \"postive\"],\n",
        "            index_to_example_function=get_example,\n",
        "        )\n",
        "\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JomdNlu0JORT"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YIJF9DWKhe9"
      },
      "source": [
        "In order to enable logging from the Hugging Face Trainer, you will need to set the `COMET_MODE` environment variable to `ONLINE`.  If you would like to log assets produced in the training run as Comet Assets, set `COMET_LOG_ASSETS=TRUE`   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kPmjwEH4NrQF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/Users/toon.weyens/.local/share/virtualenvs/comet-xykBya_K/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 200\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 25\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: COMET_MODE=ONLINE\n",
            "env: COMET_LOG_ASSETS=TRUE\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.com/toonweyens/comet-examples-transfomers-trainer/a5b73b145fc847bc92739bc2dc2c94ad\n",
            "\n",
            "Automatic Comet.ml online logging enabled\n",
            "100%|██████████| 25/25 [07:33<00:00, 17.86s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "\n",
            "100%|██████████| 25/25 [09:55<00:00, 17.86s/it]Saving model checkpoint to ./results/checkpoint-25\n",
            "Configuration saved in ./results/checkpoint-25/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6770908832550049, 'eval_accuracy': 0.575, 'eval_f1': 0.47005829358770534, 'eval_precision': 0.7090090090090091, 'eval_recall': 0.5580929487179487, 'eval_runtime': 142.6947, 'eval_samples_per_second': 1.402, 'eval_steps_per_second': 0.175, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./results/checkpoint-25/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 25/25 [09:56<00:00, 17.86s/it]Logging checkpoints. This may take time.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 602.5312, 'train_samples_per_second': 0.332, 'train_steps_per_second': 0.041, 'train_loss': 0.6857193756103516, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/toonweyens/comet-examples-transfomers-trainer/a5b73b145fc847bc92739bc2dc2c94ad\n",
            "COMET INFO:   Metrics [count] (min, max):\n",
            "COMET INFO:     epoch                    : 1.0\n",
            "COMET INFO:     eval_accuracy            : 0.575\n",
            "COMET INFO:     eval_f1                  : 0.47005829358770534\n",
            "COMET INFO:     eval_loss                : 0.6770908832550049\n",
            "COMET INFO:     eval_precision           : 0.7090090090090091\n",
            "COMET INFO:     eval_recall              : 0.5580929487179487\n",
            "COMET INFO:     eval_runtime             : 142.6947\n",
            "COMET INFO:     eval_samples_per_second  : 1.402\n",
            "COMET INFO:     eval_steps_per_second    : 0.175\n",
            "COMET INFO:     loss [3]                 : (0.660984218120575, 0.7102903723716736)\n",
            "COMET INFO:     total_flos               : 26493479731200.0\n",
            "COMET INFO:     train_loss               : 0.6857193756103516\n",
            "COMET INFO:     train_runtime            : 602.5312\n",
            "COMET INFO:     train_samples_per_second : 0.332\n",
            "COMET INFO:     train_steps_per_second   : 0.041\n",
            "COMET INFO:   Parameters:\n",
            "COMET INFO:     args/_n_gpu                             : 0\n",
            "COMET INFO:     args/_no_sync_in_gradient_accumulation  : True\n",
            "COMET INFO:     args/_setup_devices                     : cpu\n",
            "COMET INFO:     args/adafactor                          : False\n",
            "COMET INFO:     args/adam_beta1                         : 0.9\n",
            "COMET INFO:     args/adam_beta2                         : 0.999\n",
            "COMET INFO:     args/adam_epsilon                       : 1e-08\n",
            "COMET INFO:     args/auto_find_batch_size               : False\n",
            "COMET INFO:     args/bf16                               : False\n",
            "COMET INFO:     args/bf16_full_eval                     : False\n",
            "COMET INFO:     args/data_seed                          : None\n",
            "COMET INFO:     args/dataloader_drop_last               : False\n",
            "COMET INFO:     args/dataloader_num_workers             : 0\n",
            "COMET INFO:     args/dataloader_pin_memory              : True\n",
            "COMET INFO:     args/ddp_bucket_cap_mb                  : None\n",
            "COMET INFO:     args/ddp_find_unused_parameters         : None\n",
            "COMET INFO:     args/debug                              : []\n",
            "COMET INFO:     args/deepspeed                          : None\n",
            "COMET INFO:     args/device                             : cpu\n",
            "COMET INFO:     args/disable_tqdm                       : False\n",
            "COMET INFO:     args/do_eval                            : True\n",
            "COMET INFO:     args/do_predict                         : False\n",
            "COMET INFO:     args/do_train                           : True\n",
            "COMET INFO:     args/eval_accumulation_steps            : None\n",
            "COMET INFO:     args/eval_batch_size                    : 8\n",
            "COMET INFO:     args/eval_delay                         : 0\n",
            "COMET INFO:     args/eval_steps                         : 25\n",
            "COMET INFO:     args/evaluation_strategy                : IntervalStrategy.STEPS\n",
            "COMET INFO:     args/fp16                               : False\n",
            "COMET INFO:     args/fp16_backend                       : auto\n",
            "COMET INFO:     args/fp16_full_eval                     : False\n",
            "COMET INFO:     args/fp16_opt_level                     : O1\n",
            "COMET INFO:     args/fsdp                               : []\n",
            "COMET INFO:     args/fsdp_min_num_params                : 0\n",
            "COMET INFO:     args/fsdp_transformer_layer_cls_to_wrap : None\n",
            "COMET INFO:     args/full_determinism                   : False\n",
            "COMET INFO:     args/gradient_accumulation_steps        : 1\n",
            "COMET INFO:     args/gradient_checkpointing             : False\n",
            "COMET INFO:     args/greater_is_better                  : None\n",
            "COMET INFO:     args/group_by_length                    : False\n",
            "COMET INFO:     args/half_precision_backend             : auto\n",
            "COMET INFO:     args/hub_model_id                       : None\n",
            "COMET INFO:     args/hub_private_repo                   : False\n",
            "COMET INFO:     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
            "COMET INFO:     args/hub_token                          : None\n",
            "COMET INFO:     args/ignore_data_skip                   : False\n",
            "COMET INFO:     args/include_inputs_for_metrics         : False\n",
            "COMET INFO:     args/jit_mode_eval                      : False\n",
            "COMET INFO:     args/label_names                        : None\n",
            "COMET INFO:     args/label_smoothing_factor             : 0.0\n",
            "COMET INFO:     args/learning_rate                      : 5e-05\n",
            "COMET INFO:     args/length_column_name                 : length\n",
            "COMET INFO:     args/load_best_model_at_end             : False\n",
            "COMET INFO:     args/local_process_index                : 0\n",
            "COMET INFO:     args/local_rank                         : -1\n",
            "COMET INFO:     args/log_level                          : -1\n",
            "COMET INFO:     args/log_level_replica                  : -1\n",
            "COMET INFO:     args/log_on_each_node                   : True\n",
            "COMET INFO:     args/logging_dir                        : ./results/runs/Sep05_20-09-53_Toons-MBP.lan\n",
            "COMET INFO:     args/logging_first_step                 : False\n",
            "COMET INFO:     args/logging_nan_inf_filter             : True\n",
            "COMET INFO:     args/logging_steps                      : 500\n",
            "COMET INFO:     args/logging_strategy                   : IntervalStrategy.STEPS\n",
            "COMET INFO:     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
            "COMET INFO:     args/max_grad_norm                      : 1.0\n",
            "COMET INFO:     args/max_steps                          : -1\n",
            "COMET INFO:     args/metric_for_best_model              : None\n",
            "COMET INFO:     args/mp_parameters                      : \n",
            "COMET INFO:     args/n_gpu                              : 0\n",
            "COMET INFO:     args/no_cuda                            : False\n",
            "COMET INFO:     args/num_train_epochs                   : 1\n",
            "COMET INFO:     args/optim                              : OptimizerNames.ADAMW_HF\n",
            "COMET INFO:     args/output_dir                         : ./results\n",
            "COMET INFO:     args/overwrite_output_dir               : True\n",
            "COMET INFO:     args/parallel_mode                      : ParallelMode.NOT_PARALLEL\n",
            "COMET INFO:     args/past_index                         : -1\n",
            "COMET INFO:     args/per_device_eval_batch_size         : 8\n",
            "COMET INFO:     args/per_device_train_batch_size        : 8\n",
            "COMET INFO:     args/per_gpu_eval_batch_size            : None\n",
            "COMET INFO:     args/per_gpu_train_batch_size           : None\n",
            "COMET INFO:     args/place_model_on_device              : True\n",
            "COMET INFO:     args/prediction_loss_only               : False\n",
            "COMET INFO:     args/process_index                      : 0\n",
            "COMET INFO:     args/push_to_hub                        : False\n",
            "COMET INFO:     args/push_to_hub_model_id               : None\n",
            "COMET INFO:     args/push_to_hub_organization           : None\n",
            "COMET INFO:     args/push_to_hub_token                  : None\n",
            "COMET INFO:     args/ray_scope                          : last\n",
            "COMET INFO:     args/remove_unused_columns              : True\n",
            "COMET INFO:     args/report_to                          : ['comet_ml', 'tensorboard']\n",
            "COMET INFO:     args/resume_from_checkpoint             : None\n",
            "COMET INFO:     args/run_name                           : ./results\n",
            "COMET INFO:     args/save_on_each_node                  : False\n",
            "COMET INFO:     args/save_steps                         : 25\n",
            "COMET INFO:     args/save_strategy                      : IntervalStrategy.STEPS\n",
            "COMET INFO:     args/save_total_limit                   : 10\n",
            "COMET INFO:     args/seed                               : 42\n",
            "COMET INFO:     args/sharded_ddp                        : []\n",
            "COMET INFO:     args/should_log                         : True\n",
            "COMET INFO:     args/should_save                        : True\n",
            "COMET INFO:     args/skip_memory_metrics                : True\n",
            "COMET INFO:     args/tf32                               : None\n",
            "COMET INFO:     args/torchdynamo                        : None\n",
            "COMET INFO:     args/tpu_metrics_debug                  : False\n",
            "COMET INFO:     args/tpu_num_cores                      : None\n",
            "COMET INFO:     args/train_batch_size                   : 8\n",
            "COMET INFO:     args/use_ipex                           : False\n",
            "COMET INFO:     args/use_legacy_prediction_loop         : False\n",
            "COMET INFO:     args/warmup_ratio                       : 0.0\n",
            "COMET INFO:     args/warmup_steps                       : 0\n",
            "COMET INFO:     args/weight_decay                       : 0.0\n",
            "COMET INFO:     args/world_size                         : 1\n",
            "COMET INFO:     args/xpu_backend                        : None\n",
            "COMET INFO:     config/_auto_class                      : None\n",
            "COMET INFO:     config/_name_or_path                    : distilbert-base-uncased\n",
            "COMET INFO:     config/activation                       : gelu\n",
            "COMET INFO:     config/add_cross_attention              : False\n",
            "COMET INFO:     config/architectures                    : ['DistilBertForMaskedLM']\n",
            "COMET INFO:     config/attention_dropout                : 0.1\n",
            "COMET INFO:     config/attribute_map                    : {'hidden_size': 'dim', 'num_attention_heads': 'n_heads', 'num_hidden_layers': 'n_layers'}\n",
            "COMET INFO:     config/bad_words_ids                    : None\n",
            "COMET INFO:     config/bos_token_id                     : None\n",
            "COMET INFO:     config/chunk_size_feed_forward          : 0\n",
            "COMET INFO:     config/cross_attention_hidden_size      : None\n",
            "COMET INFO:     config/decoder_start_token_id           : None\n",
            "COMET INFO:     config/dim                              : 768\n",
            "COMET INFO:     config/diversity_penalty                : 0.0\n",
            "COMET INFO:     config/do_sample                        : False\n",
            "COMET INFO:     config/dropout                          : 0.1\n",
            "COMET INFO:     config/early_stopping                   : False\n",
            "COMET INFO:     config/encoder_no_repeat_ngram_size     : 0\n",
            "COMET INFO:     config/eos_token_id                     : None\n",
            "COMET INFO:     config/exponential_decay_length_penalty : None\n",
            "COMET INFO:     config/finetuning_task                  : None\n",
            "COMET INFO:     config/forced_bos_token_id              : None\n",
            "COMET INFO:     config/forced_eos_token_id              : None\n",
            "COMET INFO:     config/hidden_dim                       : 3072\n",
            "COMET INFO:     config/id2label                         : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
            "COMET INFO:     config/initializer_range                : 0.02\n",
            "COMET INFO:     config/is_composition                   : False\n",
            "COMET INFO:     config/is_decoder                       : False\n",
            "COMET INFO:     config/is_encoder_decoder               : False\n",
            "COMET INFO:     config/label2id                         : {'LABEL_0': 0, 'LABEL_1': 1}\n",
            "COMET INFO:     config/length_penalty                   : 1.0\n",
            "COMET INFO:     config/max_length                       : 20\n",
            "COMET INFO:     config/max_position_embeddings          : 512\n",
            "COMET INFO:     config/min_length                       : 0\n",
            "COMET INFO:     config/model_type                       : distilbert\n",
            "COMET INFO:     config/n_heads                          : 12\n",
            "COMET INFO:     config/n_layers                         : 6\n",
            "COMET INFO:     config/name_or_path                     : distilbert-base-uncased\n",
            "COMET INFO:     config/no_repeat_ngram_size             : 0\n",
            "COMET INFO:     config/num_beam_groups                  : 1\n",
            "COMET INFO:     config/num_beams                        : 1\n",
            "COMET INFO:     config/num_labels                       : 2\n",
            "COMET INFO:     config/num_return_sequences             : 1\n",
            "COMET INFO:     config/output_attentions                : False\n",
            "COMET INFO:     config/output_hidden_states             : False\n",
            "COMET INFO:     config/output_scores                    : False\n",
            "COMET INFO:     config/pad_token_id                     : 0\n",
            "COMET INFO:     config/prefix                           : None\n",
            "COMET INFO:     config/problem_type                     : None\n",
            "COMET INFO:     config/pruned_heads                     : {}\n",
            "COMET INFO:     config/qa_dropout                       : 0.1\n",
            "COMET INFO:     config/remove_invalid_values            : False\n",
            "COMET INFO:     config/repetition_penalty               : 1.0\n",
            "COMET INFO:     config/return_dict                      : True\n",
            "COMET INFO:     config/return_dict_in_generate          : False\n",
            "COMET INFO:     config/sep_token_id                     : None\n",
            "COMET INFO:     config/seq_classif_dropout              : 0.2\n",
            "COMET INFO:     config/sinusoidal_pos_embds             : False\n",
            "COMET INFO:     config/task_specific_params             : None\n",
            "COMET INFO:     config/temperature                      : 1.0\n",
            "COMET INFO:     config/tf_legacy_loss                   : False\n",
            "COMET INFO:     config/tie_encoder_decoder              : False\n",
            "COMET INFO:     config/tie_weights_                     : True\n",
            "COMET INFO:     config/tie_word_embeddings              : True\n",
            "COMET INFO:     config/tokenizer_class                  : None\n",
            "COMET INFO:     config/top_k                            : 50\n",
            "COMET INFO:     config/top_p                            : 1.0\n",
            "COMET INFO:     config/torch_dtype                      : None\n",
            "COMET INFO:     config/torchscript                      : False\n",
            "COMET INFO:     config/transformers_version             : 4.10.0.dev0\n",
            "COMET INFO:     config/typical_p                        : 1.0\n",
            "COMET INFO:     config/use_bfloat16                     : False\n",
            "COMET INFO:     config/use_return_dict                  : True\n",
            "COMET INFO:     config/vocab_size                       : 30522\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     asset                    : 9 (766.35 MB)\n",
            "COMET INFO:     confusion-matrix         : 1\n",
            "COMET INFO:     environment details      : 1\n",
            "COMET INFO:     filename                 : 1\n",
            "COMET INFO:     git metadata             : 1\n",
            "COMET INFO:     git-patch (uncompressed) : 1 (152.37 KB)\n",
            "COMET INFO:     installed packages       : 1\n",
            "COMET INFO:     model graph              : 1\n",
            "COMET INFO:     notebook                 : 1\n",
            "COMET INFO:     source_code              : 1\n",
            "COMET INFO: ---------------------------\n",
            "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
            "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
            "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
            "COMET INFO: The Python SDK has 10800 seconds to finish before aborting...\n",
            "COMET INFO: Still uploading 2 file(s), remaining 760.40 MB/766.49 MB\n",
            "COMET INFO: Still uploading 2 file(s), remaining 696.97 MB/766.49 MB, Throughput 4.21 MB/s, ETA ~166s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 634.72 MB/766.49 MB, Throughput 4.12 MB/s, ETA ~154s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 572.05 MB/766.49 MB, Throughput 4.16 MB/s, ETA ~138s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 533.58 MB/766.49 MB, Throughput 2.55 MB/s, ETA ~209s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 469.38 MB/766.49 MB, Throughput 4.26 MB/s, ETA ~111s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 407.02 MB/766.49 MB, Throughput 4.14 MB/s, ETA ~99s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 342.56 MB/766.49 MB, Throughput 4.28 MB/s, ETA ~81s\n",
            "COMET INFO: Still uploading 2 file(s), remaining 299.92 MB/766.49 MB, Throughput 2.83 MB/s, ETA ~106s\n",
            "COMET INFO: Still uploading 1 file(s), remaining 263.26 MB/766.49 MB, Throughput 2.43 MB/s, ETA ~109s\n",
            "COMET INFO: Still uploading 1 file(s), remaining 199.89 MB/766.49 MB, Throughput 4.21 MB/s, ETA ~48s\n",
            "COMET INFO: Still uploading 1 file(s), remaining 139.69 MB/766.49 MB, Throughput 4.00 MB/s, ETA ~35s\n",
            "COMET INFO: Still uploading 1 file(s), remaining 101.63 MB/766.49 MB, Throughput 2.52 MB/s, ETA ~41s\n",
            "COMET INFO: Still uploading 1 file(s), remaining 64.84 MB/766.49 MB, Throughput 2.44 MB/s, ETA ~27s\n",
            "COMET INFO: Still uploading 1 file(s), remaining 36.52 MB/766.49 MB, Throughput 1.88 MB/s, ETA ~20s\n",
            "COMET INFO: All files uploaded, waiting for confirmation they have been all received\n",
            "100%|██████████| 25/25 [13:50<00:00, 33.22s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=25, training_loss=0.6857193756103516, metrics={'train_runtime': 602.5312, 'train_samples_per_second': 0.332, 'train_steps_per_second': 0.041, 'train_loss': 0.6857193756103516, 'epoch': 1.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%env COMET_MODE=ONLINE\n",
        "%env COMET_LOG_ASSETS=TRUE\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    seed=SEED,\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_total_limit=10,\n",
        "    save_steps=25,\n",
        "    per_device_train_batch_size=8,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Comet with Hugging Face",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('comet-xykBya_K')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d6d842fb6e54b9408236d63269e7fd7cf2e5749bf572172d303f17aeebef6f9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
